# README: Hackathon SmartDoc.ai

## L'Equipe 2
- BADOLO Christian Thomas
- KINDO Harouna 
- NABI Daniel

Ce projet a √©t√© r√©alis√© dans le cadre du hackathon SmartDoc.ai, ayant pour objectif principal le traitement de documents financiers au format PDF pour en extraire uniquement les contenus pertinents √† l'aide d'outils NLP. Voici une description des √©tapes r√©alis√©es lors des diff√©rentes parties cet exercice.

---
# Premiere partie (using Google Vision API)

## 1. Traitement des Donn√©es OCR

### Fonctionnalit√©s Utilis√©es :
La fonction **`produce_brut()`** fournie dans le fichier `helper.py` (que l'on a gard√© comme telle) a √©t√© utilis√©e telle quelle pour transformer les fichiers JSON obtenus √† partir de l'OCR (Google Vision API) en un tableau Excel structurant les blocs textuels extraits des rapports SFCR. Cette fonction constitue la base des analyses effectu√©es dans les √©tapes suivantes.



## 2. D√©tection et Lab√©lisation des Contenus

### Objectifs :
L'objectif principal √©tait de classifier automatiquement les blocs textuels extraits des rapports SFCR en trois cat√©gories :
- **Inutile** : Contenus non pertinents comme les bas de page, hauts de page et tableaux.
- **Paragraphe** : Contenus informatifs pertinents pour le corps principal des rapports.
- **Titre** : Grands titres ou sous-titres d√©limitant les diff√©rentes sections des rapports.

### Approche Technique :
Pour cette √©tape, une fonction nomm√©e **`label_content(df, thresholds=None)`** a √©t√© d√©velopp√©e dans le fichier [notebook](google_vision_api/report_cleaning.ipynb). Elle repose sur des seuils d√©finis pour diff√©rencier les cat√©gories de contenu.

#### Fonctionnement de `label_content()` :
1. **Seuils Utilis√©s :**
   - Position verticale (`pos_y`) pour les en-t√™tes et pieds de page.
   - Nombre de caract√®res (`chars`) pour distinguer titres et paragraphes.
   - Taille des caract√®res et hauteur des blocs (`char_size`, `height`) pour identifier le contenu des tableaux.

2. **Classification :** Chaque bloc textuel est √©valu√© selon ces seuils pour √™tre classifi√© en "Inutile", "Titre" ou "Paragraphe". Par exemple :
   - Si la position verticale est proche des bords (haut ou bas de page), il est marqu√© comme "Inutile".
   - Si le nombre de caract√®res est tr√®s faible, il est marqu√© comme "Titre".
   - Si le nombre de caract√®res est √©lev√©, il est consid√©r√© comme "Paragraphe".

### Filtrage et G√©n√©ration des Fichiers Texte :
Une fois la lab√©lisation effectu√©e, les donn√©es inutiles sont filtr√©es pour ne conserver que les titres et paragraphes pertinents. Le contenu r√©sultant est ensuite sauvegard√© dans un fichier texte suivant une organisation claire :
- Les titres et paragraphes sont regroup√©s par page.
- Une ligne de s√©paration est ajout√©e entre les pages pour une meilleure lisibilit√©.

#### Exemple de Code :
Le fichier g√©n√©r√© est produit √† l'aide de la fonction suivante :
```python
# Fonction pour g√©n√©rer un fichier texte organis√©
 def generate_text(dataframe, filename):
     with open(filename, 'w', encoding='utf-8') as f:
         current_page = None
         for _, row in dataframe.iterrows():
             if current_page is None or row['num_page'] != current_page:
                 if current_page is not None:
                     f.write("\n" + "="*50 + "\n")  # S√©parateur pour une nouvelle page
                 current_page = row['num_page']
                 f.write(f"\nPage {current_page}\n")

             if row['Label'] == 'Titre':
                 f.write(f"\n{row['text']}\n")
             elif row['Label'] == 'Paragraphe':
                 f.write(f"{row['text']}\n")

             f.write("\n")
```

### R√©sultats :
- **Classification Automatis√©e :** Les blocs textuels sont correctement identifi√©s et class√©s.
- **Fichiers Lisibles :** Les fichiers texte produits sont clairs et organis√©s par page avec une distinction nette entre les titres et les paragraphes.

### ‚ö†Ô∏è L'on a [ici](google_vision_api/text) 4 fichers `.txt` de l'extraction des 4 [rappors PDF](data/pdfs) 


### Analyse :

La lab√©lisation a √©t√© faite selon des crit√®res qu'il est difficile de g√©n√©raliser.
L'√©tape de lab√©lisation, bien que fonctionnelle, pr√©sente certaines limites dues √† la difficult√© de d√©finir des crit√®res universels applicables √† tous les types de rapports SFCR. En effet, les seuils d√©finis pour la position verticale, la taille des caract√®res et le nombre de caract√®res sont bas√©s sur des observations sp√©cifiques, ce qui peut entra√Æner des erreurs ou des ambigu√Øt√©s dans certaines situations.



## Conclusion :
Ces √©tapes ont permis d'√©tablir une base solide pour l'analyse des rapports SFCR en filtrant efficacement le contenu utile. Les techniques de traitement et de lab√©lisation d√©velopp√©es ici pr√©parent √† la deuxi√®me partie de l'exercice, centr√©e sur l'impl√©mentation d'une architecture RAG.


## 3. Bonnus : Extraction lisible des informations des tableaux


### Objectif  
L'objectif de cette partie √©tait de d√©tecter et extraire automatiquement les tableaux pr√©sents dans des fichiers PDF, puis de convertir leur contenu en texte structur√© tout en pr√©servant la disposition tabulaire. Le code a √©t√© devollop√© dans ce [notebook](tables/table_detection_and_extraction.ipynb)


### √âtapes de la M√©thodologie

1. **D√©tection des Tableaux**  
   - **Mod√®le Utilis√© :** Un mod√®le [YOLOüåê](https://huggingface.co/foduucom/table-detection-and-extraction) a √©t√© employ√© pour d√©tecter les tableaux dans les pages du PDF.  
   - **Processus :** Les pages des PDF sont converties en images. Le mod√®le analyse ces images pour rep√©rer les zones contenant des tableaux et les d√©coupe en sous-images correspondant √† chaque tableau.  
   - **Param√®tres Cl√©s :** Des seuils de confiance (confidence score) et IoU (Intersection over Union) ont √©t√© ajust√©s pour optimiser la pr√©cision de la d√©tection des tableaux.

2. **Extraction des Images des Tableaux**  
   - Une fois d√©tect√©s, les tableaux sont extraits sous forme d'images individuelles et sauvegard√©s dans un r√©pertoire. Chaque image repr√©sente un tableau unique trouv√© dans le document.

3. **Conversion des Images en Texte**  
   - **Outil Utilis√© :** [Tesseract-OCRüåê](https://github.com/tesseract-ocr/tesseract) a √©t√© utilis√© pour convertir le contenu des images en texte lisible et structur√©.  
   - **Pr√©traitement :** Les images des tableaux ont √©t√© redimensionn√©es et converties en RGB pour am√©liorer la qualit√© de l'extraction du texte.  
   - **Structure Conserv√©e :** Une analyse des positions et des blocs textuels a permis de recr√©er la structure tabulaire originale dans le format texte.

4. **R√©sultats Structur√©s**  
   - Le contenu textuel des tableaux est format√© dans des formats exploitables (dans notre cas du texte) pour faciliter les analyses ult√©rieures par les mod√®les.


### R√©sultats  
Cette m√©thodologie a permis :  
- Une d√©tection pr√©cise des tableaux dans des documents PDF complexes.  
- Une extraction fid√®le du contenu tabulaire, avec une pr√©servation de la structure.  
- Une pr√©paration des donn√©es sous une forme facilement exploitable pour des besoins d'analyse ou d'int√©gration.

#### Exemple SFCR [COVEA](data/pdfs/sfcr_covea_2022.PDF) : image d√©tect√©e puis text d√©tect√©

- Apres d√©tection des tables par YOLO (page 89)
![Page 89](images/page_89_apres_YOLO.jpg)

- Apres d√©tection du text dans l'image
![Page 89](images/page_89_apres_Tessaract.png)

### Analyse :
Le mod√®le YOLO permet une detection et extraction syst√©matique sous forme d'image de toutes les tables dans les diff√©rents PDF. La difficult√© apparente se trouve au niveau de l'extraction des tables de ces images (dans le cas de l'utilisation de mod√®le lite non multimodale)
La prochaine √©tape cruciale consiste √† int√©grer les tableaux extraits dans le texte lab√©lis√©, afin de reconstituer une structure documentaire coh√©rente proche de l'original. L'objectif est de fusionner intelligemment les r√©sultats de la lab√©lisation (titres et paragraphes) avec les tableaux transform√©s en texte. Cette int√©gration n√©cessiterait de d√©velopper un algorithme capable d'identifier la position contextuelle de chaque tableau dans le document original, en se basant sur les titres adjacents, la pagination, et le contenu environnant. Il s'agirait de cr√©er un m√©canisme qui repositionne chaque tableau √† son emplacement initial, en respectant la hi√©rarchie des sections et la logique narrative du document. Cette approche permettrait de g√©n√©rer un document reconstitu√© o√π les tableaux seraient r√©ins√©r√©s de mani√®re organique, pr√©servant ainsi la structure et la coh√©rence du rapport SFCR initial.

Cette √©tape de fusion pourrait √™tre r√©alis√©e ult√©rieurement.

---

# Premiere partie (using  Llama Parser)

## 1. Exctraction avec  LlamaParse
Vu les limites apparentes de la m√©thode d'extraction avec `Google Vision`, nous avons effectu√© un benchmark qui a abouti √† la solution de `LlamaCloud` : [Llama Parser](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/). LlamaParse est un parseur de documents sur le march√© sp√©cialement con√ßu pour les am√©liorer les RAG. Cette solution a permis d'extraire de mani√®re fid√®le les informations de divers PDF sous forme de `Markdown`. En utilisant `LlamaParse`, nous avons pu surmonter les d√©fis li√©s √† l'extraction de contenu complexe, tels que les tableaux, les diagrammes et l'ordre de lecture, en obtenant des r√©sultats plus pr√©cis et mieux structur√©s que ceux offerts par les mod√®les multimodaux traditionnels. Gr√¢ce √† son approche hybride, LlamaParse a r√©duit les erreurs d'extraction, tout en offrant une meilleure gestion du contenu visuel et textuel.

### ‚ö†Ô∏è [Code LlamaParse](llama_parser/Hackathon_LlamaParse.ipynb)

## 2. R√©sulats
### ‚ö†Ô∏è L'on a [ici](llama_parser/markdown) 4 fichers `.md` de l'extraction des 4 [rappors PDF](data/pdfs) 


### Parsing flow

![Flow](images/architecture.gif)

---

# Deuxieme partie

## üèóÔ∏è Architecture Technique D√©taill√©e

### 1. Choix du Mod√®le de Langage (LLM)(voir [code](rag_architecture/response.py))

La s√©lection de Google Generative AI (Gemini), et plus particuli√®rement de la version [1.5 Flash](https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash), r√©sulte d'une analyse approfondie des besoins sp√©cifiques de notre cas d'usage. Ce mod√®le se distingue par sa capacit√© exceptionnelle √† comprendre et √† traiter des contextes financiers complexes. Sa ma√Ætrise du fran√ßais, combin√©e √† des performances de pointe en analyse de documents techniques, en fait un choix strat√©gique.

Les points forts de Gemini incluent sa capacit√© √† :
- Maintenir la coh√©rence dans l'interpr√©tation de documents longs et techniques
- G√©rer efficacement les nuances du langage financier
- Fournir des r√©ponses structur√©es et professionnelles
- S'adapter rapidement √† diff√©rents styles de rapports financiers

### 2. Strat√©gie Avanc√©e de Chunking(voir [code](rag_architecture/inital_vector.py))

La m√©thode de d√©coupage des documents (chunking) repr√©sente un √©l√©ment crucial de notre architecture RAG. Utilisant RecursiveCharacterTextSplitter, nous avons d√©velopp√© une approche qui va au-del√† du simple d√©coupage m√©canique des documents.

Notre strat√©gie vise √† :
- Pr√©server l'int√©grit√© s√©mantique des sections
- Maintenir un contexte suffisamment large pour une compr√©hension profonde
- Permettre une recherche de similarit√© pr√©cise
- Minimiser la fragmentation des informations cruciales

Avec des chunks de 100 000 caract√®res et un chevauchement de 200 caract√®res, nous garantissons une transition en douceur entre les segments, assurant qu'aucun d√©tail important ne soit perdu lors de l'analyse.

### 3. Vectorisation S√©mantique de Pointe(voir [code](rag_architecture/inital_vector.py))

Le mod√®le d'embedding de Google (`models/embedding-001`) transforme chaque segment de texte en un vecteur math√©matique riche en informations s√©mantiques. Cette transformation permet une recherche de similarit√© qui va bien au-del√† des correspondances litt√©rales, en capturant les nuances et les relations conceptuelles entre diff√©rentes parties du document.

Les avantages de cette approche incluent :
- Une compr√©hension contextuelle profonde
- La capacit√© de relier des concepts financiers apparemment disparates
- Une pr√©cision accrue dans la recherche de segments pertinents

### 4. Moteur de Recherche Vectoriel FAISS(voir [code](rag_architecture/inital_vector.py))

FAISS (Facebook AI Similarity Search) repr√©sente la colonne vert√©brale de notre syst√®me de recherche. Cette biblioth√®que open-source d√©velopp√©e par Facebook permet des recherches de similarit√© ultrarapides, m√™me sur de tr√®s grands ensembles de donn√©es.

Son impl√©mentation nous permet de :
- Indexer rapidement des milliers de pages de rapports financiers
- Effectuer des recherches de similarit√© en quelques millisecondes
- G√©rer efficacement des volumes importants de donn√©es vectoris√©es

## üîç Processus de Recherche et G√©n√©ration

Notre cha√Æne de traitement int√®gre plusieurs √©tapes sophistiqu√©es pour garantir des r√©ponses de haute qualit√© :

1. **Pr√©traitement Intelligent**: D√©coupage et vectorisation des documents
2. **Recherche S√©mantique**: Identification des segments les plus pertinents
3. **G√©n√©ration Contextualis√©e**: Production de r√©ponses pr√©cises et professionnelles

Le prompt engineering joue un r√¥le crucial, guidant le mod√®le avec des instructions d√©taill√©es pour :
- Utiliser exclusivement le contexte fourni
- Maintenir une structure de r√©ponse professionnelle
- G√©rer explicitement les cas o√π l'information est incompl√®te ou absente

### RAG architecture
![](images/architecture.jpg)


---

# Interface Streamlit(voir [code](main.py))


## üé• D√©mo de mon projet

Voici une d√©monstration vid√©o :

<iframe src="https://www.loom.com/share/158c4b96c70c447685bd8416f31841e5?sid=b5102413-d26f-4136-b670-e049b11bcdfc">
</iframe>



---

# √âtapes pour utiliser ce projet : 

1. **Cloner le d√©p√¥t**  
   T√©l√©chargez le projet sur votre machine en clonant le d√©p√¥t GitHub avec la commande suivante :  
   ```bash
   git clone https://github.com/zenitsu93/Hackathon_Smartdoc.ai.git
   ```

2. **Se d√©placer dans le r√©pertoire du projet**  
   Entrez dans le r√©pertoire du projet clon√© :  
   ```bash
   cd Hackathon_Smartdoc.ai
   ```

3. **Cr√©er un environnement virtuel (.venv)**  
   Cr√©ez un environnement virtuel Python pour isoler les d√©pendances du projet :  
   ```bash
   python -m venv .venv
   ```

4. **Activer l‚Äôenvironnement virtuel**  
   - Sur **Windows** :  
     ```bash
     .venv\Scripts\activate
     ```
   - Sur **Mac/Linux** :  
     ```bash
     source .venv/bin/activate
     ```

5. **Installer les d√©pendances**  
   Installez les biblioth√®ques n√©cessaires √† partir du fichier `requirements.txt` :  
   ```bash
   pip install -r requirements.txt
   ```

6. **Cr√©er un fichier `.env` pour les variables d‚Äôenvironnement**  
   - Cr√©ez un fichier `.env` dans le r√©pertoire principal du projet.  
   - Ajoutez-y votre cl√© API Google :  
     ```
     GOOGLE_API_KEY=VotreCl√©API
     ```
     Remplacez `VotreCl√©API` par la cl√© API que vous avez obtenue depuis la console [Google Cloud](https://aistudio.google.com/apikey).

7. **Lancer l'application avec Streamlit**  
   Ex√©cutez la commande suivante pour d√©marrer l'application :  
   ```bash
   streamlit run main.py
   ```

---
