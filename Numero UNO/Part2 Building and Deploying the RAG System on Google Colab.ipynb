{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building and Deploying the RAG System on Google Colab\n"
      ],
      "metadata": {
        "id": "KnwJxWiPZcml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup Google Colab Environment\n"
      ],
      "metadata": {
        "id": "6Rq0YS3QZ9y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " install the required libraries"
      ],
      "metadata": {
        "id": "2m7Qv_x1Z_iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu sentence-transformers openai pdfplumber plotly transformers"
      ],
      "metadata": {
        "id": "345r4BkvaEPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Setup API Keys and Environment Variables"
      ],
      "metadata": {
        "id": "QMIcTHp2aG8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save your work and access files from Google Drive"
      ],
      "metadata": {
        "id": "fhrH-xi1aKaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9a3D1fqGacgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For online functionality, add your OpenAI API key. If offline, skip this step."
      ],
      "metadata": {
        "id": "cslNEwv3af1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'your_openai_api_key'"
      ],
      "metadata": {
        "id": "jP8PC5uQaka5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement RAG Components"
      ],
      "metadata": {
        "id": "A-S8p8_EapiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to split large documents into manageable chunks"
      ],
      "metadata": {
        "id": "tw418gWxarPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def chunk_document(text, chunk_size=800, chunk_overlap=100):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        "    )\n",
        "    return text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "qjQxouuwauPF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Online Embeddings (OpenAI)"
      ],
      "metadata": {
        "id": "fRI-eQGiawy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "def create_online_embeddings(chunks):\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
        "    return embeddings.embed_documents(chunks)"
      ],
      "metadata": {
        "id": "-zmvk4xiazXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Offline Embeddings (Sentence-BERT)"
      ],
      "metadata": {
        "id": "Qoaigul2a1kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def create_offline_embeddings(chunks):\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    return [model.encode(chunk) for chunk in chunks]"
      ],
      "metadata": {
        "id": "_lPDD7jza4Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Store Indexing with FAISS"
      ],
      "metadata": {
        "id": "SzAP63Ila7qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "def create_vector_store(embeddings, chunks):\n",
        "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "2NWMX0wka_Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question Answering Pipeline"
      ],
      "metadata": {
        "id": "yIJZLHypbGTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Online QA"
      ],
      "metadata": {
        "id": "xLMcelmNbHAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def online_response_pipeline(question, vector_store):\n",
        "    retriever = vector_store.as_retriever()\n",
        "    llm = OpenAI(model='gpt-4', openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
        "    qa_chain = RetrievalQA(llm=llm, retriever=retriever)\n",
        "    return qa_chain.run(question)"
      ],
      "metadata": {
        "id": "yBwRxZqUbJNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Offline QA"
      ],
      "metadata": {
        "id": "qu_DELhlbMD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def offline_response_pipeline(question, vector_store):\n",
        "    retriever = vector_store.as_retriever()\n",
        "    context_chunks = retriever.retrieve(question, top_k=5)\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    answer = qa_pipeline({\"context\": context, \"question\": question})\n",
        "    return answer['answer']"
      ],
      "metadata": {
        "id": "Nd2PoLTobOWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Integrate PDF Parsing"
      ],
      "metadata": {
        "id": "6XrJVyjabTR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload and Parse PDFs"
      ],
      "metadata": {
        "id": "0iW19qh4bURR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def parse_pdf(file_path):\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
        "    return text"
      ],
      "metadata": {
        "id": "NrtMaM4gbW4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the files module to upload SFCR PDFs:"
      ],
      "metadata": {
        "id": "BOUnqoIvbZPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for file_name in uploaded.keys():\n",
        "    text = parse_pdf(file_name)"
      ],
      "metadata": {
        "id": "EEBKedR7bcD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Visualize Data"
      ],
      "metadata": {
        "id": "E8a_3Rqdbh0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Plotly for interactive visualizations:"
      ],
      "metadata": {
        "id": "R05aO9dNbioK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "def visualize_data(data):\n",
        "    fig = px.bar(data, x='Year', y='Solvency Ratio', title='Solvency Ratios Over Time')\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "bJNo6EaSblqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Deploy Interactive Chatbot"
      ],
      "metadata": {
        "id": "A3UvH7Gabn5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Chat Interface"
      ],
      "metadata": {
        "id": "O8bJeP-Ebuhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as display\n",
        "\n",
        "def chatbot_interface():\n",
        "    document = input(\"Paste the SFCR document text: \")\n",
        "    question = input(\"Enter your question: \")\n",
        "    chunks = chunk_document(document)\n",
        "\n",
        "    use_offline = input(\"Use offline mode? (yes/no): \").strip().lower() == 'yes'\n",
        "\n",
        "    if use_offline:\n",
        "        embeddings = create_offline_embeddings(chunks)\n",
        "        vector_store = create_vector_store(embeddings, chunks)\n",
        "        answer = offline_response_pipeline(question, vector_store)\n",
        "    else:\n",
        "        embeddings = create_online_embeddings(chunks)\n",
        "        vector_store = create_vector_store(embeddings, chunks)\n",
        "        answer = online_response_pipeline(question, vector_store)\n",
        "\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "chatbot_interface()"
      ],
      "metadata": {
        "id": "8bK9Fcf-bwMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Save and Share Results"
      ],
      "metadata": {
        "id": "gKQT-gUWbzSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/SFCR_Results.txt', 'w') as f:\n",
        "    f.write(answer)"
      ],
      "metadata": {
        "id": "vBU_1kLYb0CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "This step-by-step guide walks you through building the RAG system on Google Colab. It includes:\n",
        "\n",
        "Setting up embeddings and vector stores.\n",
        "\n",
        "Parsing and processing PDF reports.\n",
        "\n",
        "Visualizing results.\n",
        "\n",
        "Deploying an interactive chatbot."
      ],
      "metadata": {
        "id": "OqFxh5wlb7Xy"
      }
    }
  ]
}